{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 48: Global Activation Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset used:- [ALL-IDB:Acute Lymphoblastic Leukemia Image Database for Image Processing](https://homes.di.unimi.it/scotti/all/)\n",
    "Follow the instructions provided in the linked website to download the dataset. After downloading, extract the files to the current directory (same folder as your codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import tqdm\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "print(torch.__version__) # This code has been updated for PyTorch 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datapath = 'ALL_IDB2/img/'\n",
    "listing = os.listdir(Datapath) \n",
    "random.shuffle(listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL_IDB2 dataset has 260 images in total\n",
    "TrainImages = torch.FloatTensor(200,3,224,224)\n",
    "TrainLabels = torch.LongTensor(200)\n",
    "TestImages = torch.FloatTensor(60,3,224,224)\n",
    "TestLabels = torch.LongTensor(60)\n",
    "\n",
    "# First 200 images are used for training and the remaining 60 for testing\n",
    "img_no = 0\n",
    "for file in listing:\n",
    "    im=Image.open(Datapath + file)\n",
    "    im = im.resize((224,224))\n",
    "    im = np.array(im)   \n",
    "    if img_no < 200:\n",
    "        TrainImages[img_no] = torch.from_numpy(im).transpose(0,2).unsqueeze(0)\n",
    "        TrainLabels[img_no] = int(listing[img_no][6:7])\n",
    "    else:\n",
    "        TestImages[img_no - 200] = torch.from_numpy(im).transpose(0,2).unsqueeze(0)\n",
    "        TestLabels[img_no - 200] = int(listing[img_no][6:7])\n",
    "    img_no = img_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TrainImages.size())\n",
    "print(TrainLabels.size())\n",
    "print(TestImages.size())\n",
    "print(TestLabels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check availability of GPU\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False # Uncomment in case of GPU memory error\n",
    "if use_gpu:\n",
    "    print('GPU is available!')\n",
    "    device = \"cuda\"\n",
    "    pinMem = True\n",
    "else:\n",
    "    print('GPU is not available!')\n",
    "    device = \"cpu\"\n",
    "    pinMem = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating pytorch dataset\n",
    "trainDataset = TensorDataset(TrainImages, TrainLabels)\n",
    "testDataset = TensorDataset(TestImages, TestLabels)\n",
    "# Creating dataloader\n",
    "BatchSize = 32\n",
    "trainLoader = DataLoader(trainDataset, batch_size=BatchSize, shuffle=True,num_workers=4, pin_memory=pinMem)\n",
    "testLoader = DataLoader(testDataset, batch_size=BatchSize, shuffle=False,num_workers=4, pin_memory=pinMem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ResNet18\n",
    "net = models.resnet18(pretrained=True)\n",
    "num_ftrs = net.fc.in_features\n",
    "net.fc = nn.Linear(num_ftrs, 2)\n",
    "print(net)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss() # Negative Log-Likelihood\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3 , momentum=0.9) # Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterations = 15\n",
    "trainLoss = []\n",
    "trainAcc = []\n",
    "testLoss = []\n",
    "testAcc = []\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(iterations):\n",
    "    epochStart = time.time()\n",
    "    runningLoss = 0   \n",
    "    runningCorr = 0\n",
    "    net.train() # For training\n",
    "    for data in tqdm.tqdm_notebook(trainLoader):\n",
    "        inputs,labels = data\n",
    "        inputs, labels = inputs.to(device), labels.long().to(device)          \n",
    "       \n",
    "        inputs = inputs/255.0\n",
    "        # Feed-forward input data through the network\n",
    "        outputs = net(inputs)\n",
    "        # Compute loss/error\n",
    "        loss = criterion(F.log_softmax(outputs,dim=1), labels) \n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # Initialize gradients to zero\n",
    "        optimizer.zero_grad()                  \n",
    "        # Backpropagate loss and compute gradients\n",
    "        loss.backward()\n",
    "        # Update the network parameters\n",
    "        optimizer.step()\n",
    "        # Accumulate loss per batch\n",
    "        runningLoss += loss.item()  \n",
    "        # Accumuate correct predictions per batch\n",
    "        runningCorr += (predicted == labels.data).sum()\n",
    "    avgTrainLoss = runningLoss/(200.0/BatchSize)\n",
    "    avgTrainAcc = 100*float(runningCorr)/200.0\n",
    "    trainLoss.append(avgTrainLoss)\n",
    "    trainAcc.append(avgTrainAcc)\n",
    "    \n",
    "    # Evaluating performance on test set for each epoch\n",
    "    net.eval() # For testing\n",
    "    test_runningCorr = 0\n",
    "    test_runningLoss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testLoader:\n",
    "            inputs,labels = data\n",
    "            inputs, labels = inputs.to(device), labels.long().to(device)  \n",
    "            inputs = inputs/255\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)    \n",
    "             # Compute loss/error\n",
    "            loss = criterion(F.log_softmax(outputs,dim=1), labels)      \n",
    "            # Accumulate loss per batch\n",
    "            test_runningLoss += loss.item()  \n",
    "            # Accumuate correct predictions per batch\n",
    "            test_runningCorr += (predicted == labels.data).sum()\n",
    "    avgTestLoss = test_runningLoss/(60.0/BatchSize)\n",
    "    avgTestAcc = 100*float(test_runningCorr)/60.0\n",
    "    testAcc.append(avgTestAcc)\n",
    "    testLoss.append(avgTestLoss)\n",
    "        \n",
    "    # Plotting Loss vs Epochs\n",
    "    fig1 = plt.figure(1)        \n",
    "    plt.plot(range(epoch+1),trainLoss,'r--',label='train')        \n",
    "    plt.plot(range(epoch+1),testLoss,'g--',label='test')        \n",
    "    if epoch==0:\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')    \n",
    "    # Plotting testing accuracy vs Epochs\n",
    "    fig2 = plt.figure(2)        \n",
    "    plt.plot(range(epoch+1),trainAcc,'r-',label='train') \n",
    "    plt.plot(range(epoch+1),testAcc,'g-',label='test')        \n",
    "    if epoch==0:\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')    \n",
    "    \n",
    "    epochEnd = time.time()-epochStart\n",
    "    print('At Iteration: {:.0f} /{:.0f}  ;  Training Loss: {:.6f} ; Training Acc: {:.3f} ; Time consumed: {:.0f}m {:.0f}s '\\\n",
    "          .format(epoch + 1,iterations,avgTrainLoss,avgTrainAcc,epochEnd//60,epochEnd%60))\n",
    "    print('At Iteration: {:.0f} /{:.0f}  ;  Testing Loss: {:.6f} ; Testing Acc: {:.3f} ; Time consumed: {:.0f}m {:.0f}s '\\\n",
    "          .format(epoch + 1,iterations,avgTestLoss,avgTestAcc,epochEnd//60,epochEnd%60))\n",
    "end = time.time()-start\n",
    "print('Training completed in {:.0f}m {:.0f}s'.format(end//60,end%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting the convolutional layers of the network\n",
    "conv_net = nn.Sequential(*list(net.children())[:-2])\n",
    "print(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying weights of the final layer for obtaining the segmented output\n",
    "weights = copy.deepcopy(net.fc.weight.data)\n",
    "if use_gpu:\n",
    "    weights = weights.cpu()\n",
    "weights = weights.numpy()\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading one sample image for testing\n",
    "testPath = 'ALL_IDB1/img/'\n",
    "testImages = os.listdir(testPath)\n",
    "img1 = plt.imread(testPath+testImages[0])\n",
    "testInput = torch.from_numpy(img1).transpose(0,2).transpose(1,2).unsqueeze(0).float().to(device)\n",
    "# Feed-forward\n",
    "out = conv_net(testInput)  \n",
    "\n",
    "# Visualization\n",
    "if use_gpu:\n",
    "    out_np = out.squeeze(0).data.cpu().numpy()\n",
    "else:\n",
    "    out_np = out.squeeze(0).data.numpy()\n",
    "\n",
    "mask1 = np.ones(out_np.shape)\n",
    "for n1 in range(512):\n",
    "    mask1[n1] = weights[0,n1]*mask1[n1]\n",
    "outImg1 = np.sum(np.multiply(mask1,out_np),axis=0)\n",
    "\n",
    "# Averaged activation map\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(np.sum(out_np,axis=0)/512,cmap='gray')\n",
    "# Weighted-sum activation map\n",
    "plt.subplot(122)\n",
    "plt.imshow(outImg1,cmap='gray')\n",
    "\n",
    "# Activation maps chosen at random\n",
    "plt.figure()\n",
    "randIdx = np.random.randint(0,511,4)\n",
    "plt.subplot(141)\n",
    "plt.imshow(out_np[randIdx[0]],cmap='gray')\n",
    "plt.subplot(142)\n",
    "plt.imshow(out_np[randIdx[1]],cmap='gray')\n",
    "plt.subplot(143)\n",
    "plt.imshow(out_np[randIdx[2]],cmap='gray')\n",
    "plt.subplot(144)\n",
    "plt.imshow(out_np[randIdx[3]],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
