{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 22: Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from skimage.measure import compare_ssim as ssim #Structural similarity index\n",
    "\n",
    "print(torch.__version__) # This code has been updated for PyTorch 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "BatchSize = 100\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./MNIST', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BatchSize,\n",
    "                                          shuffle=True, num_workers=4) # Creating dataloader\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./MNIST', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BatchSize,\n",
    "                                         shuffle=False, num_workers=4) # Creating dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check availability of GPU\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print('GPU is available!')\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print('GPU is not available!')\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Losses\n",
    "## Define the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 400),\n",
    "            nn.Tanh())\n",
    "        self.decoder = nn.Sequential(          \n",
    "            nn.Linear(400, 28*28),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "net1 = autoencoder().to(device) # Network to be trained using MSE loss\n",
    "net2 = autoencoder().to(device) # Network to be trained using L1 loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(model,optimizer,criterion,datainput,label):\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(datainput)\n",
    "    loss = criterion(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "learning_rate = 1\n",
    "\n",
    "optimizer1 = optim.Adam(net1.parameters(), lr=1e-4) # Network to be trained using MSE loss\n",
    "optimizer2 = optim.Adam(net2.parameters(), lr=1e-4) # Network to be trained using L1 loss\n",
    "\n",
    "criterion1 = nn.MSELoss()\n",
    "criterion2 = nn.L1Loss()\n",
    "\n",
    "Plotssim1 = []\n",
    "Plotssim2 = []\n",
    "plotLoss1 = []\n",
    "plotLoss2 = []\n",
    "\n",
    "testImage = testloader.dataset[0][0]\n",
    "testinputs = testImage.view(-1, 28*28).to(device)\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "    running_loss1 = 0.0\n",
    "    running_loss2 = 0.0\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 28*28).to(device)              \n",
    "      \n",
    "        trainLoss1  = Train(net1,optimizer1,criterion1,inputs,inputs) # MSE Loss\n",
    "        trainLoss2 = Train(net2,optimizer2,criterion2,inputs,inputs)  # L1 Loss  \n",
    "        \n",
    "        running_loss1 += trainLoss1\n",
    "        running_loss2 += trainLoss2\n",
    "    plotLoss1.append(running_loss1/(i+1))\n",
    "    plotLoss2.append(running_loss2/(i+1))       \n",
    "\n",
    "    net1.eval()  \n",
    "    net2.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = net1(testinputs.to(device))    \n",
    "        if use_gpu:\n",
    "            outputs = outputs.cpu()\n",
    "            testinputs = testinputs.cpu()\n",
    "        ssim1 = ssim(outputs.data.view(28,28).numpy(),testinputs.data.view(28,28).numpy())\n",
    "    \n",
    "        outputs = net2(testinputs.to(device))\n",
    "        if use_gpu:\n",
    "            outputs = outputs.cpu()\n",
    "            testinputs = testinputs.cpu()\n",
    "        ssim2 = ssim(outputs.data.view(28,28).numpy(),testinputs.data.view(28,28).numpy())\n",
    "\n",
    "    Plotssim1.append(float(ssim1))\n",
    "    Plotssim2.append(float(ssim2))\n",
    "    \n",
    "    print('At Epoch '+str(epoch+1))\n",
    "    print('With MSELoss: Loss = {:.6f}, SSIM Index = {:.5f} '.format(running_loss1/(i+1),float(ssim1)))\n",
    "    print('With L1Loss: Loss = {:.6f}, SSIM Index = {:.5f} '.format(running_loss2/(i+1),float(ssim2)))\n",
    "\n",
    "fig = plt.figure()        \n",
    "plt.plot(range(epoch+1),plotLoss1,'r-',label='Mean Square Error')\n",
    "plt.plot(range(epoch+1),plotLoss2,'g-',label='L1 Loss')   \n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')  \n",
    "\n",
    "fig = plt.figure()         \n",
    "plt.plot(range(epoch+1),Plotssim1,'r-',label='SSIM Index (MSE)')\n",
    "plt.plot(range(epoch+1),Plotssim2,'g-',label='SSIM Index (L1)')      \n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Testing SSIM') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Loss\n",
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.Layer1 = nn.Sequential(\n",
    "            nn.Linear(28*28, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 256),\n",
    "            nn.ReLU())\n",
    "        self.Layer2 = nn.Sequential(\n",
    "            nn.Linear(256, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Layer1(x)\n",
    "        x = self.Layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = NeuralNet().to(device) # Network to be trained using cross-entropy loss\n",
    "net2 = NeuralNet().to(device) # Network to be trained using NLL loss\n",
    "net3 = NeuralNet().to(device) # Network to be trained using multi-margin loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(model,optimizer,criterion,datainput,label,lossType):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(datainput)\n",
    "    if lossType == 'NLL':\n",
    "        loss = criterion(F.log_softmax(output,dim=1), label)\n",
    "    else:\n",
    "        loss = criterion(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "learning_rate = 0.1\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.NLLLoss()\n",
    "criterion3 = nn.MultiMarginLoss()\n",
    "\n",
    "optimizer1 = optim.Adam(net1.parameters(), lr=1e-4) # Network to be trained using cross-entropy loss\n",
    "optimizer2 = optim.Adam(net2.parameters(), lr=1e-4) # Network to be trained using NLL loss\n",
    "optimizer3 = optim.Adam(net3.parameters(), lr=1e-4) # Network to be trained using multi-margin loss\n",
    "\n",
    "Plotacc1 = []\n",
    "Plotacc2 = []\n",
    "Plotacc3 = []\n",
    "\n",
    "plotLoss1 = []\n",
    "plotLoss2 = []\n",
    "plotLoss3 = []\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "    correct1 = 0\n",
    "    correct2 = 0\n",
    "    correct3 = 0\n",
    "    runningLoss1 = 0\n",
    "    runningLoss2 = 0\n",
    "    runningLoss3 = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(trainloader, 0): # i-> batch count\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.view(-1, 28*28).to(device), labels.to(device)    \n",
    "        \n",
    "        trainLoss1 = Train(net1,optimizer1,criterion1,inputs,labels,lossType='CE')\n",
    "        trainLoss2 = Train(net2,optimizer2,criterion2,inputs,labels,lossType='NLL')   \n",
    "        trainLoss3 = Train(net3,optimizer3,criterion3,inputs,labels,lossType='MM')    \n",
    "\n",
    "        runningLoss1 += trainLoss1\n",
    "        runningLoss2 += trainLoss2\n",
    "        runningLoss3 += trainLoss3\n",
    "   \n",
    "    runningLoss1 = runningLoss1/(i+1)\n",
    "    runningLoss2 = runningLoss2/(i+1)\n",
    "    runningLoss3 = runningLoss3/(i+1)          \n",
    "   \n",
    "    plotLoss1.append(runningLoss1)\n",
    "    plotLoss2.append(runningLoss2)\n",
    "    plotLoss3.append(runningLoss3)\n",
    "    \n",
    "    net1.eval()\n",
    "    net2.eval()\n",
    "    net3.eval()\n",
    "    with torch.no_grad():    \n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.view(-1, 28*28).to(device), labels.to(device)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            outputs = net1(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct1 += (predicted == labels).sum()\n",
    "\n",
    "            outputs = net2(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct2 += (predicted == labels).sum()\n",
    "\n",
    "            outputs = net3(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct3 += (predicted == labels).sum()\n",
    "\n",
    "    Plotacc1.append(100*float(correct1)/float(total))\n",
    "    Plotacc2.append(100*float(correct2)/float(total))\n",
    "    Plotacc3.append(100*float(correct3)/float(total))\n",
    "    \n",
    "    print('At Epoch '+str(epoch+1))\n",
    "    print('With CrossEntropyLoss: Loss = {:.6f} , Acc = {:.4f}%'.format(runningLoss1,100*float(correct1)/float(total)))\n",
    "    print('With NegativeLogLikelihoodLoss: Loss = {:.6f} , Acc = {:.4f}%'.format(runningLoss2,100*float(correct2)/float(total)))\n",
    "    print('With MultiMarginLoss: Loss = {:.6f} , Acc = {:.4f}%\\n'.format(runningLoss3,100*float(correct3)/float(total)))\n",
    "    \n",
    "fig = plt.figure()        \n",
    "plt.plot(range(epoch+1),plotLoss1,'r-',label='Cross Entropy Loss')\n",
    "plt.plot(range(epoch+1),plotLoss2,'g-',label='Negative Log Likelihood Loss')   \n",
    "plt.plot(range(epoch+1),plotLoss3,'b-',label='Multi Margin Loss')  \n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')  \n",
    "    \n",
    "fig = plt.figure()        \n",
    "plt.plot(range(epoch+1),Plotacc1,'r-',label='Cross Entropy Loss')\n",
    "plt.plot(range(epoch+1),Plotacc2,'g-',label='Negative Log Likelihood Loss')   \n",
    "plt.plot(range(epoch+1),Plotacc3,'b-',label='Multi Margin Loss')  \n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Testing Accuracy')  \n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
