{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10: Classification with Multilayer Perceptron\n",
    "\n",
    "Dataset used: [Cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import tqdm\n",
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "print(torch.__version__) # This code has been updated for PyTorch 1.0.0\n",
    "\n",
    "\n",
    "savePath = 'lecture10_output/'\n",
    "if not os.path.isdir(savePath):\n",
    "    os.makedirs(savePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading saved features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trainFeats.pckl\", \"rb\") as f:\n",
    "    trainFeats = pickle.load(f)\n",
    "with open(\"trainLabel.pckl\", \"rb\") as f:\n",
    "    trainLabel = pickle.load(f)\n",
    "    \n",
    "with open(\"testFeats.pckl\", \"rb\") as f:\n",
    "    testFeats = pickle.load(f)\n",
    "with open(\"testLabel.pckl\", \"rb\") as f:\n",
    "    testLabel = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "    def __init__(self,n_channels): #n_channels => length of feature vector\n",
    "        super(mlp, self).__init__()\n",
    "        self.L1 = nn.Linear(n_channels,6) #Mapping from input to hidden layer       \n",
    "        self.L2 = nn.Linear(6,10) #Mapping from hidden layer to output\n",
    "    def forward(self,x): #x => Input\n",
    "        x = self.L1(x) #Feed-forward  \n",
    "        x = F.relu(x) #Sigmoid non-linearity\n",
    "        x = self.L2(x) #Feed-forward           \n",
    "        x = F.softmax(x,dim=1) #Sigmoid non-linearity\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating 1-hot label vectors\n",
    "trainLabel2 = np.zeros((50000,10))\n",
    "for d1 in range(trainLabel.shape[0]):\n",
    "    trainLabel2[d1,trainLabel[d1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking availability of GPU\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print('GPU is available!')\n",
    "    device = \"cuda\"\n",
    "    pinMem = True\n",
    "else:\n",
    "    print('GPU is not available!')\n",
    "    device = \"cpu\"\n",
    "    pinMem = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating pytorch dataset from the feature matices\n",
    "trainDataset = TensorDataset(torch.from_numpy(trainFeats), torch.from_numpy(trainLabel2))\n",
    "testDataset = TensorDataset(torch.from_numpy(testFeats), torch.from_numpy(testLabel))\n",
    "# Creating dataloader\n",
    "trainLoader = DataLoader(trainDataset, batch_size=64, shuffle=True,num_workers=4, pin_memory=pinMem)\n",
    "testLoader = DataLoader(testDataset, batch_size=64, shuffle=False,num_workers=4, pin_memory=pinMem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining function for training the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definining the training routine\n",
    "def train_model(model,criterion,num_epochs,learning_rate):\n",
    "        start = time.time()\n",
    "        train_loss = [] #List for saving the loss per epoch     \n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epochStartTime = time.time()\n",
    "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "\n",
    "            running_loss = 0.0           \n",
    "            # Loading data in batches\n",
    "            batch = 0\n",
    "            for data in tqdm.tqdm_notebook(trainLoader):\n",
    "                inputs,labels = data\n",
    "                \n",
    "                inputs, labels = inputs.float().to(device),labels.float().to(device)\n",
    "                \n",
    "                # Initializing model gradients to zero\n",
    "                model.zero_grad() \n",
    "                # Data feed-forward through the network\n",
    "                outputs = model(inputs)\n",
    "                # Predicted class is the one with maximum probability\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                # Finding the MSE\n",
    "                loss = criterion(outputs, labels)\n",
    "                # Accumulating the loss for each batch\n",
    "                running_loss += loss.item()\n",
    "                # Backpropaging the error\n",
    "                if batch == 0:\n",
    "                    totalLoss = loss\n",
    "                    totalPreds = preds\n",
    "                    batch += 1                    \n",
    "                else:\n",
    "                    totalLoss += loss\n",
    "                    totalPreds = torch.cat((totalPreds,preds),0)  \n",
    "                    batch += 1\n",
    "                    \n",
    "            totalLoss = totalLoss/batch\n",
    "            totalLoss.backward()\n",
    "            \n",
    "            # Updating the model parameters\n",
    "            for f in model.parameters():\n",
    "                f.data.sub_(f.grad.data * learning_rate)                \n",
    "           \n",
    "            epoch_loss = running_loss/50000  #Total loss for one epoch\n",
    "            train_loss.append(epoch_loss) #Saving the loss over epochs for plotting the graph           \n",
    "            \n",
    "            print('Epoch loss: {:.6f}'.format(epoch_loss))\n",
    "            epochTimeEnd = time.time()-epochStartTime\n",
    "            print('Epoch complete in {:.0f}m {:.0f}s'.format(\n",
    "            epochTimeEnd // 60, epochTimeEnd % 60))\n",
    "            print('-' * 25)\n",
    "            # Plotting Loss vs Epochs\n",
    "            fig1 = plt.figure(1)        \n",
    "            plt.plot(range(epoch+1),train_loss,'r--',label='train')        \n",
    "            if epoch==0:\n",
    "                plt.legend(loc='upper left')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.title('Plot of training loss vs epochs')\n",
    "            fig1.savefig(savePath+'mlp_lossPlot.png')             \n",
    "\n",
    "        time_elapsed = time.time() - start\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model initialization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_length = 2+5+2\n",
    "model = mlp(feat_length).to(device) # Initilaizing the model\n",
    "criterion = nn.MSELoss() \n",
    "model = train_model(model,criterion,num_epochs=100,learning_rate=10) # Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding testing accuracy\n",
    "test_running_corr = 0\n",
    "# Loading data in batches\n",
    "batches = 0\n",
    "\n",
    "model.eval() # Testing the model in evaluation mode\n",
    "\n",
    "for tsData in tqdm.tqdm_notebook(testLoader):\n",
    "    inputs,_ = tsData\n",
    "    \n",
    "    inputs = inputs.float().to(device)\n",
    "    \n",
    "    with torch.no_grad(): # No back-propagation during testing; gradient computation is not required\n",
    "    \n",
    "        # Feedforward train data batch through model\n",
    "        output = model(inputs) \n",
    "        # Predicted class is the one with maximum probability\n",
    "        _,preds = output.data.max(1)    \n",
    "        if batches==0:\n",
    "            totalPreds = preds\n",
    "            batches = 1\n",
    "        else:\n",
    "            totalPreds = torch.cat((totalPreds,preds),0)\n",
    "\n",
    "ts_corr = np.sum(np.equal(totalPreds.cpu().numpy(),testLabel))\n",
    "ts_acc = ts_corr/testLabel.shape[0]\n",
    "print('Testing accuracy = '+str(ts_acc*100)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
